###### Hold on a millisecond
# British ethicists are challenging justice by algorithm 
![image](images/20190810_BRD002_0.jpg) 
> print-edition iconPrint edition | Britain | Aug 10th 2019 
FOR A TIME, the humans seemed to be winning. The Metropolitan Police’s squad of “super-recognisers” was lauded for its uncanny ability to recall faces in video footage. The officers spotted sex-offenders in crowds of thousands and nabbed a thief who had pinched more than £100,000 ($122,000) of luxury goods. Technology was not nearly as reliable. One of the super-recognisers identified 180 of the 4,000 suspects captured on camera during riots in 2011, whereas software spotted only one. “Computers are no match for the super-recognisers,” boasted the unit’s boss. 
Now the computers are fighting back. Many of the 43 police forces in England and Wales are experimenting with algorithmic technology that could render the copper’s nose redundant. Several use programs to predict where and when crimes are likely to occur. Cambridge University helped Durham Constabulary design an algorithm to estimate the likelihood of a suspect reoffending. It helps the authorities decide whether someone should be granted bail or qualify for rehabilitation as an alternative to prosecution. At least one force is keen to install microphones on “smart lamp-posts” to gather intelligence in crowds. Even the cherished super-recognisers will be outdone once facial-recognition algorithms improve, predicts Rick Muir of the Police Foundation, a think-tank. 
Not everyone is pleased. On July 3rd academics published a critical assessment of Scotland Yard’s pilots of automatic facial-recognition technology, querying their legal basis and casting doubt on whether people caught on camera could be said to have given their informed consent. Judges in Cardiff are weighing the lawfulness of similar trials by South Wales Police. And in June the Law Society, which represents solicitors, raised concerns about the “general and concerning lack of openness or transparency” in the police’s use of algorithms. 
Several wonk shops are being set up to examine the ethics of algorithmic technology, including one at Oxford backed by a £150m donation from Stephen Schwarzman, the boss of Blackstone, a private-equity firm. The Centre for Data Ethics and Innovation, a new government-funded agency, is likely to propose a code of conduct to regulate cops’ use of technology, says Roger Taylor, its chairman, who acknowledges the need to act “very quickly” to close any gaps in oversight. 
Critics make four arguments. First, the technology does not work terribly well. In the pilots in London, only eight of the 42 matches made by facial-recognition software were correct. Second, the systems are a disproportionate response to crime. In an era when data-protection regulations govern the mailing list of a pizza joint, civil-liberties campaigners question why the national police database holds 12.5m images in its gallery—including images of an undisclosed number of people who have neither been charged with an offence nor consented to the use of their pictures. 
Third, it could prove discriminatory. Since some facial-recognition technology is best at identifying white faces, it could throw up more erroneous “matches” for non-white people, making them more likely to be the subject of unwarranted police attention. Finally, it risks compromising the principle that justice must be seen to be done. If suspects cannot understand how an algorithm reached a decision, they might find it harder to challenge. 
Yet none of these hurdles is insurmountable. The technology will improve. Britons already accept lots of surveillance: although most people do not shoplift, they are used to being monitored by CCTV cameras. A poll published in May suggests most Londoners are happy for the police to use facial-recognition software, especially to spot serious criminals. A powerful regulator ought to be able to strike the right balance and allay fears of bias. 
And although humans can give reasons for their decisions, there is plenty of evidence suggesting they are influenced by unconscious biases, points out Lawrence Sherman of Cambridge University. It ought to be easier to scrutinise and challenge the processes of one algorithm than the decisions of thousands of cops and judges. “There’s nothing less transparent than the human mind,” says Mr Sherman. ■ 
